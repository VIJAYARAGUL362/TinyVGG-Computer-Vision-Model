{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/).\n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA).\n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ],
      "metadata": {
        "id": "Vex99np2wFVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GaeYzOTLwWh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "DNwZLMbCzJLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ],
      "metadata": {
        "id": "FSFX7tc1w-en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. manufacturing\n",
        "2. self driving car\n",
        "3. face recongnition"
      ],
      "metadata": {
        "id": "VyWRkvWGbCXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find."
      ],
      "metadata": {
        "id": "oBK-WI6YxDYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is when the model learns more about the training dataset and memorizes it instead of generalizing it making it do\n",
        "perform worst on the test dataset"
      ],
      "metadata": {
        "id": "d1rxD6GObCqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each.\n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ],
      "metadata": {
        "id": "XeYFEqw8xK26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Increasing the no of samples\n",
        "2. using a dropout layers\n",
        "3. reducing the no of epochs"
      ],
      "metadata": {
        "id": "ocvOdWKcbEKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ],
      "metadata": {
        "id": "DKdEEFEqxM-8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ],
      "metadata": {
        "id": "lvf-3pODxXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "\n",
        "training_dataset = datasets.FashionMNIST(\n",
        "    \"data\",\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform = None,\n",
        "    download = True\n",
        ")\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    \"data\",\n",
        "    train = False,\n",
        "    transform = ToTensor(),\n",
        "    target_transform = None,\n",
        "    download = True\n",
        ")"
      ],
      "metadata": {
        "id": "SHjeuN81bHza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ],
      "metadata": {
        "id": "qxZW-uAbxe_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "rand_num = random.randint(1,len(training_dataset))\n",
        "image = training_dataset.data[rand_num]\n",
        "label = training_dataset.targets[rand_num].item()\n",
        "classes_name = training_dataset.classes\n",
        "plt.title(classes_name[label])\n",
        "plt.imshow(image,cmap =\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QVFsYi1PbItE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ],
      "metadata": {
        "id": "JAPDzW0wxhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "    training_dataset,\n",
        "    batch_size = 32,\n",
        "    shuffle = True,\n",
        "    )\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = 32,\n",
        "    shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "ALA6MPcFbJXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ],
      "metadata": {
        "id": "bCCVfXk5xjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "RpU_wqFO2EGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates a tinyVGG model\n",
        "    params:\n",
        "    input_units (int)\n",
        "    hidden_units (int)\n",
        "    output_path (int)\n",
        "    \"\"\"\n",
        "    def __init__(self,input_units,hidden_units,output_units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1,\n",
        "                      out_channels = hidden_units,\n",
        "                      kernel_size = 3,\n",
        "                      stride = 1,\n",
        "                      padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = hidden_units,\n",
        "                      out_channels = hidden_units,\n",
        "                      kernel_size = 3,\n",
        "                      stride = 1,\n",
        "                      padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size = 2\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = hidden_units,\n",
        "                      out_channels= hidden_units,\n",
        "                      kernel_size = 3,\n",
        "                      stride = 1,\n",
        "                      padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels = hidden_units,\n",
        "                      kernel_size = 3,\n",
        "                      stride = 1,\n",
        "                      padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2)\n",
        "        )\n",
        "\n",
        "        self.conv_block_3 = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features = 490,out_features=output_units),\n",
        "        )\n",
        "\n",
        "    def forward(self,X:torch.Tensor):\n",
        "        x = self.conv_block_1(X)\n",
        "        # print(x.shape)\n",
        "        x = self.conv_block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.conv_block_3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1  = TinyVGG(input_units = 1,\n",
        "                   hidden_units = 10,\n",
        "                   output_units = len(classes_name))"
      ],
      "metadata": {
        "id": "8_oxZwr-68bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.to(device)"
      ],
      "metadata": {
        "id": "-VOkuKtr7_jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ],
      "metadata": {
        "id": "sf_3zUr7xlhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing the random data to the model\n",
        "random_data = torch.rand(size=(1,1,28,28))\n",
        "print(random_data.shape)\n",
        "\n",
        "model_1(random_data.to(device))"
      ],
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LODSS FUNCTION AND OPTIMIZER\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),lr=0.1)"
      ],
      "metadata": {
        "id": "7kf5IKJ1_VDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING THE MODEL\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "epochs = 5\n",
        "\n",
        "start_time_on_cpu = timer()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    training_accuracy = 0\n",
        "    training_loss = 0\n",
        "\n",
        "    testing_accuracy = 0\n",
        "    testing_loss = 0\n",
        "\n",
        "    # model on training mode\n",
        "    model_1.train()\n",
        "\n",
        "    for batches,(X_train,y_train) in enumerate(train_dataloader):\n",
        "\n",
        "        X_train,y_train = X_train.to(device),y_train.to(device)\n",
        "        # FORWARD PROPAGATION\n",
        "        y_pred_train = model_1(X_train)\n",
        "\n",
        "        # LOSS CALCULATION\n",
        "        train_loss = loss_fn(y_pred_train,y_train)\n",
        "        training_loss += train_loss\n",
        "\n",
        "        # ACCURACY CALCULATION\n",
        "        train_accu = accuracy_score(y_train.cpu(),y_pred_train.argmax(dim=1).cpu())\n",
        "        training_accuracy += train_accu\n",
        "\n",
        "        # OPTIMIZER STEP\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # BACKPROPAGATION\n",
        "        train_loss.backward()\n",
        "\n",
        "        # GRADIENT DESCENT\n",
        "        optimizer.step()\n",
        "\n",
        "        if batches%400 == 0:\n",
        "            print(f\"Trained on {batches*len(X_train)}/{len(train_dataloader.dataset)}\")\n",
        "\n",
        "    training_accuracy /= len(train_dataloader)\n",
        "    training_loss /= len(train_dataloader)\n",
        "\n",
        "\n",
        "    model_1.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X_test,y_test in test_dataloader:\n",
        "            X_test,y_test = X_test.to(device),y_test.to(device)\n",
        "            y_pred_test = model_1(X_test)\n",
        "            test_loss = loss_fn(y_pred_test,y_test)\n",
        "            testing_loss += test_loss\n",
        "            test_accu = accuracy_score(y_test.cpu(),y_pred_test.argmax(dim=1).cpu())\n",
        "            testing_accuracy += test_accu\n",
        "\n",
        "\n",
        "\n",
        "        testing_loss /= len(test_dataloader)\n",
        "        testing_accuracy /= len(test_dataloader)\n",
        "\n",
        "    print(f\"Train acc: {training_accuracy:.2f}, Train loss: {training_loss:.2f}|\\\n",
        "            Test acc : {testing_accuracy:.2f}, Test loss: {testing_loss:.2f}\")\n",
        "\n",
        "end_time_on_cpu =timer()\n",
        "\n",
        "total_time_on_cpu = end_time_on_cpu - start_time_on_cpu"
      ],
      "metadata": {
        "id": "SdfyQ4WzBXbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{total_time_on_cpu:.2f} seconds\")"
      ],
      "metadata": {
        "id": "BufcEgImJu5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ],
      "metadata": {
        "id": "w1CsHhPpxp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random_number = random.randint(1,len(list(test_dataloader)))\n",
        "random_batch,random_label = list(test_dataloader)[random_number]\n",
        "\n",
        "y_pred_logits = model_1(random_batch[0].unsqueeze(dim=0).to(device))\n",
        "y_pred_label = torch.softmax(y_pred_logits,dim=1).argmax(dim=1).item()\n",
        "\n",
        "pred_class = classes_name[y_pred_label]\n",
        "original_class = classes_name[random_label[0].item()]\n",
        "\n",
        "plt.imshow(random_batch[0].squeeze().cpu(),cmap=\"gray\")\n",
        "plt.title(f\"True:{original_class} |pred: {pred_class}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ],
      "metadata": {
        "id": "qQwzqlBWxrpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q torchmetrics"
      ],
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(data:torch.utils.data.DataLoader,model:torch.nn.Module,device:torch.device)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Evalute model on the data give and return the predictions labels\n",
        "    params:\n",
        "    data (torch.utils.data.DataLoader)\n",
        "    model (torch.nn.Module)\n",
        "    device (torch.device)\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    with torch.inference_mode():\n",
        "        for X,y in data:\n",
        "            X,y = X.to(device),y.to(device)\n",
        "            pred_logits = model(X)\n",
        "            pred_labels = torch.softmax(pred_logits.squeeze(),dim=0).argmax(dim=1)\n",
        "\n",
        "            predictions.append(pred_labels.cpu())\n",
        "\n",
        "\n",
        "    return torch.cat(predictions)"
      ],
      "metadata": {
        "id": "D_2ofax4EcKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFUSION MATRIX\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "preds = evaluation(test_dataloader,model_1,device)\n",
        "\n",
        "matrix = ConfusionMatrix(task=\"multiclass\",num_classes=len(classes_name))\n",
        "confmat = matrix(preds,test_dataset.targets)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=confmat.numpy(),\n",
        "                      figsize=(10,16),\n",
        "                      class_names=classes_name)"
      ],
      "metadata": {
        "id": "2nR6F7I-Dt-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ],
      "metadata": {
        "id": "lj6bDhoWxt2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d = nn.Conv2d(in_channels = 3,\n",
        "                   out_channels = 10,\n",
        "                   kernel_size = 3,\n",
        "                   stride = 4,\n",
        "                   padding = 1)\n",
        "random_data = torch.rand(size=[1,3,64,64])\n",
        "\n",
        "random_prediction = conv2d(random_data)\n",
        "random_prediction.shape"
      ],
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset.\n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been.\n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error?\n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "VHS20cNTxwSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have done the visualization\n",
        "I think there is some labelling error also we have to tune the model."
      ],
      "metadata": {
        "id": "78a8LjtdbSZj"
      }
    }
  ]
}